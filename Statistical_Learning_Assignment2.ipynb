{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Learning Assignment2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before building models and compare it, process installing libraries and splitting the data into test and training set should be preceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>default</th><th scope=col>student</th><th scope=col>balance</th><th scope=col>income</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 729.5265</td><td>44361.625</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 817.1804</td><td>12106.135</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1073.5492</td><td>31767.139</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 529.2506</td><td>35704.494</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 785.6559</td><td>38463.496</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 919.5885</td><td> 7491.559</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 825.5133</td><td>24905.227</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 808.6675</td><td>17600.451</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1161.0579</td><td>37468.529</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>   0.0000</td><td>29275.268</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td>   0.0000</td><td>21871.073</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td>1220.5838</td><td>13268.562</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 237.0451</td><td>28251.695</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 606.7423</td><td>44994.556</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1112.9684</td><td>23810.174</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 286.2326</td><td>45042.413</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>   0.0000</td><td>50265.312</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 527.5402</td><td>17636.540</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 485.9369</td><td>61566.106</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1095.0727</td><td>26464.631</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 228.9525</td><td>50500.182</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 954.2618</td><td>32457.509</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1055.9566</td><td>51317.883</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 641.9844</td><td>30466.103</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 773.2117</td><td>34353.314</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 855.0085</td><td>25211.332</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 642.9997</td><td>41473.512</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1454.8633</td><td>32189.095</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 615.7043</td><td>39376.395</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td>1119.5694</td><td>16556.070</td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td>1294.5004</td><td>25687.33 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 180.6201</td><td>20975.56 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 755.4328</td><td>14455.87 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 876.1190</td><td>37668.37 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 933.3320</td><td>26051.40 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 908.3159</td><td>21287.94 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 218.4176</td><td>25401.13 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 915.4398</td><td>16624.34 </td></tr>\n",
       "\t<tr><td>Yes      </td><td>No       </td><td>2202.4624</td><td>47287.26 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 173.2492</td><td>30697.25 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 770.0157</td><td>13684.79 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 739.4180</td><td>40656.95 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 623.5261</td><td>59441.31 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 506.6255</td><td>49861.00 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 875.2416</td><td>52861.74 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 842.9494</td><td>39957.13 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 401.3327</td><td>15332.02 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1092.9066</td><td>45479.47 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>   0.0000</td><td>41740.69 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 999.2811</td><td>20013.35 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 372.3792</td><td>25374.90 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 658.7996</td><td>54802.08 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1111.6473</td><td>45490.68 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 938.8362</td><td>56633.45 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 172.4130</td><td>14955.94 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 711.5550</td><td>52992.38 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 757.9629</td><td>19660.72 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 845.4120</td><td>58636.16 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1569.0091</td><td>36669.11 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 200.9222</td><td>16862.95 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " default & student & balance & income\\\\\n",
       "\\hline\n",
       "\t No        & No        &  729.5265 & 44361.625\\\\\n",
       "\t No        & Yes       &  817.1804 & 12106.135\\\\\n",
       "\t No        & No        & 1073.5492 & 31767.139\\\\\n",
       "\t No        & No        &  529.2506 & 35704.494\\\\\n",
       "\t No        & No        &  785.6559 & 38463.496\\\\\n",
       "\t No        & Yes       &  919.5885 &  7491.559\\\\\n",
       "\t No        & No        &  825.5133 & 24905.227\\\\\n",
       "\t No        & Yes       &  808.6675 & 17600.451\\\\\n",
       "\t No        & No        & 1161.0579 & 37468.529\\\\\n",
       "\t No        & No        &    0.0000 & 29275.268\\\\\n",
       "\t No        & Yes       &    0.0000 & 21871.073\\\\\n",
       "\t No        & Yes       & 1220.5838 & 13268.562\\\\\n",
       "\t No        & No        &  237.0451 & 28251.695\\\\\n",
       "\t No        & No        &  606.7423 & 44994.556\\\\\n",
       "\t No        & No        & 1112.9684 & 23810.174\\\\\n",
       "\t No        & No        &  286.2326 & 45042.413\\\\\n",
       "\t No        & No        &    0.0000 & 50265.312\\\\\n",
       "\t No        & Yes       &  527.5402 & 17636.540\\\\\n",
       "\t No        & No        &  485.9369 & 61566.106\\\\\n",
       "\t No        & No        & 1095.0727 & 26464.631\\\\\n",
       "\t No        & No        &  228.9525 & 50500.182\\\\\n",
       "\t No        & No        &  954.2618 & 32457.509\\\\\n",
       "\t No        & No        & 1055.9566 & 51317.883\\\\\n",
       "\t No        & No        &  641.9844 & 30466.103\\\\\n",
       "\t No        & No        &  773.2117 & 34353.314\\\\\n",
       "\t No        & No        &  855.0085 & 25211.332\\\\\n",
       "\t No        & No        &  642.9997 & 41473.512\\\\\n",
       "\t No        & No        & 1454.8633 & 32189.095\\\\\n",
       "\t No        & No        &  615.7043 & 39376.395\\\\\n",
       "\t No        & Yes       & 1119.5694 & 16556.070\\\\\n",
       "\t ... & ... & ... & ...\\\\\n",
       "\t No        & Yes       & 1294.5004 & 25687.33 \\\\\n",
       "\t No        & Yes       &  180.6201 & 20975.56 \\\\\n",
       "\t No        & No        &  755.4328 & 14455.87 \\\\\n",
       "\t No        & No        &  876.1190 & 37668.37 \\\\\n",
       "\t No        & Yes       &  933.3320 & 26051.40 \\\\\n",
       "\t No        & No        &  908.3159 & 21287.94 \\\\\n",
       "\t No        & No        &  218.4176 & 25401.13 \\\\\n",
       "\t No        & Yes       &  915.4398 & 16624.34 \\\\\n",
       "\t Yes       & No        & 2202.4624 & 47287.26 \\\\\n",
       "\t No        & No        &  173.2492 & 30697.25 \\\\\n",
       "\t No        & Yes       &  770.0157 & 13684.79 \\\\\n",
       "\t No        & No        &  739.4180 & 40656.95 \\\\\n",
       "\t No        & No        &  623.5261 & 59441.31 \\\\\n",
       "\t No        & No        &  506.6255 & 49861.00 \\\\\n",
       "\t No        & No        &  875.2416 & 52861.74 \\\\\n",
       "\t No        & No        &  842.9494 & 39957.13 \\\\\n",
       "\t No        & Yes       &  401.3327 & 15332.02 \\\\\n",
       "\t No        & No        & 1092.9066 & 45479.47 \\\\\n",
       "\t No        & No        &    0.0000 & 41740.69 \\\\\n",
       "\t No        & Yes       &  999.2811 & 20013.35 \\\\\n",
       "\t No        & No        &  372.3792 & 25374.90 \\\\\n",
       "\t No        & No        &  658.7996 & 54802.08 \\\\\n",
       "\t No        & No        & 1111.6473 & 45490.68 \\\\\n",
       "\t No        & No        &  938.8362 & 56633.45 \\\\\n",
       "\t No        & Yes       &  172.4130 & 14955.94 \\\\\n",
       "\t No        & No        &  711.5550 & 52992.38 \\\\\n",
       "\t No        & No        &  757.9629 & 19660.72 \\\\\n",
       "\t No        & No        &  845.4120 & 58636.16 \\\\\n",
       "\t No        & No        & 1569.0091 & 36669.11 \\\\\n",
       "\t No        & Yes       &  200.9222 & 16862.95 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| default | student | balance | income |\n",
       "|---|---|---|---|\n",
       "| No        | No        |  729.5265 | 44361.625 |\n",
       "| No        | Yes       |  817.1804 | 12106.135 |\n",
       "| No        | No        | 1073.5492 | 31767.139 |\n",
       "| No        | No        |  529.2506 | 35704.494 |\n",
       "| No        | No        |  785.6559 | 38463.496 |\n",
       "| No        | Yes       |  919.5885 |  7491.559 |\n",
       "| No        | No        |  825.5133 | 24905.227 |\n",
       "| No        | Yes       |  808.6675 | 17600.451 |\n",
       "| No        | No        | 1161.0579 | 37468.529 |\n",
       "| No        | No        |    0.0000 | 29275.268 |\n",
       "| No        | Yes       |    0.0000 | 21871.073 |\n",
       "| No        | Yes       | 1220.5838 | 13268.562 |\n",
       "| No        | No        |  237.0451 | 28251.695 |\n",
       "| No        | No        |  606.7423 | 44994.556 |\n",
       "| No        | No        | 1112.9684 | 23810.174 |\n",
       "| No        | No        |  286.2326 | 45042.413 |\n",
       "| No        | No        |    0.0000 | 50265.312 |\n",
       "| No        | Yes       |  527.5402 | 17636.540 |\n",
       "| No        | No        |  485.9369 | 61566.106 |\n",
       "| No        | No        | 1095.0727 | 26464.631 |\n",
       "| No        | No        |  228.9525 | 50500.182 |\n",
       "| No        | No        |  954.2618 | 32457.509 |\n",
       "| No        | No        | 1055.9566 | 51317.883 |\n",
       "| No        | No        |  641.9844 | 30466.103 |\n",
       "| No        | No        |  773.2117 | 34353.314 |\n",
       "| No        | No        |  855.0085 | 25211.332 |\n",
       "| No        | No        |  642.9997 | 41473.512 |\n",
       "| No        | No        | 1454.8633 | 32189.095 |\n",
       "| No        | No        |  615.7043 | 39376.395 |\n",
       "| No        | Yes       | 1119.5694 | 16556.070 |\n",
       "| ... | ... | ... | ... |\n",
       "| No        | Yes       | 1294.5004 | 25687.33  |\n",
       "| No        | Yes       |  180.6201 | 20975.56  |\n",
       "| No        | No        |  755.4328 | 14455.87  |\n",
       "| No        | No        |  876.1190 | 37668.37  |\n",
       "| No        | Yes       |  933.3320 | 26051.40  |\n",
       "| No        | No        |  908.3159 | 21287.94  |\n",
       "| No        | No        |  218.4176 | 25401.13  |\n",
       "| No        | Yes       |  915.4398 | 16624.34  |\n",
       "| Yes       | No        | 2202.4624 | 47287.26  |\n",
       "| No        | No        |  173.2492 | 30697.25  |\n",
       "| No        | Yes       |  770.0157 | 13684.79  |\n",
       "| No        | No        |  739.4180 | 40656.95  |\n",
       "| No        | No        |  623.5261 | 59441.31  |\n",
       "| No        | No        |  506.6255 | 49861.00  |\n",
       "| No        | No        |  875.2416 | 52861.74  |\n",
       "| No        | No        |  842.9494 | 39957.13  |\n",
       "| No        | Yes       |  401.3327 | 15332.02  |\n",
       "| No        | No        | 1092.9066 | 45479.47  |\n",
       "| No        | No        |    0.0000 | 41740.69  |\n",
       "| No        | Yes       |  999.2811 | 20013.35  |\n",
       "| No        | No        |  372.3792 | 25374.90  |\n",
       "| No        | No        |  658.7996 | 54802.08  |\n",
       "| No        | No        | 1111.6473 | 45490.68  |\n",
       "| No        | No        |  938.8362 | 56633.45  |\n",
       "| No        | Yes       |  172.4130 | 14955.94  |\n",
       "| No        | No        |  711.5550 | 52992.38  |\n",
       "| No        | No        |  757.9629 | 19660.72  |\n",
       "| No        | No        |  845.4120 | 58636.16  |\n",
       "| No        | No        | 1569.0091 | 36669.11  |\n",
       "| No        | Yes       |  200.9222 | 16862.95  |\n",
       "\n"
      ],
      "text/plain": [
       "      default student balance   income   \n",
       "1     No      No       729.5265 44361.625\n",
       "2     No      Yes      817.1804 12106.135\n",
       "3     No      No      1073.5492 31767.139\n",
       "4     No      No       529.2506 35704.494\n",
       "5     No      No       785.6559 38463.496\n",
       "6     No      Yes      919.5885  7491.559\n",
       "7     No      No       825.5133 24905.227\n",
       "8     No      Yes      808.6675 17600.451\n",
       "9     No      No      1161.0579 37468.529\n",
       "10    No      No         0.0000 29275.268\n",
       "11    No      Yes        0.0000 21871.073\n",
       "12    No      Yes     1220.5838 13268.562\n",
       "13    No      No       237.0451 28251.695\n",
       "14    No      No       606.7423 44994.556\n",
       "15    No      No      1112.9684 23810.174\n",
       "16    No      No       286.2326 45042.413\n",
       "17    No      No         0.0000 50265.312\n",
       "18    No      Yes      527.5402 17636.540\n",
       "19    No      No       485.9369 61566.106\n",
       "20    No      No      1095.0727 26464.631\n",
       "21    No      No       228.9525 50500.182\n",
       "22    No      No       954.2618 32457.509\n",
       "23    No      No      1055.9566 51317.883\n",
       "24    No      No       641.9844 30466.103\n",
       "25    No      No       773.2117 34353.314\n",
       "26    No      No       855.0085 25211.332\n",
       "27    No      No       642.9997 41473.512\n",
       "28    No      No      1454.8633 32189.095\n",
       "29    No      No       615.7043 39376.395\n",
       "30    No      Yes     1119.5694 16556.070\n",
       "...   ...     ...     ...       ...      \n",
       "9971  No      Yes     1294.5004 25687.33 \n",
       "9972  No      Yes      180.6201 20975.56 \n",
       "9973  No      No       755.4328 14455.87 \n",
       "9974  No      No       876.1190 37668.37 \n",
       "9975  No      Yes      933.3320 26051.40 \n",
       "9976  No      No       908.3159 21287.94 \n",
       "9977  No      No       218.4176 25401.13 \n",
       "9978  No      Yes      915.4398 16624.34 \n",
       "9979  Yes     No      2202.4624 47287.26 \n",
       "9980  No      No       173.2492 30697.25 \n",
       "9981  No      Yes      770.0157 13684.79 \n",
       "9982  No      No       739.4180 40656.95 \n",
       "9983  No      No       623.5261 59441.31 \n",
       "9984  No      No       506.6255 49861.00 \n",
       "9985  No      No       875.2416 52861.74 \n",
       "9986  No      No       842.9494 39957.13 \n",
       "9987  No      Yes      401.3327 15332.02 \n",
       "9988  No      No      1092.9066 45479.47 \n",
       "9989  No      No         0.0000 41740.69 \n",
       "9990  No      Yes      999.2811 20013.35 \n",
       "9991  No      No       372.3792 25374.90 \n",
       "9992  No      No       658.7996 54802.08 \n",
       "9993  No      No      1111.6473 45490.68 \n",
       "9994  No      No       938.8362 56633.45 \n",
       "9995  No      Yes      172.4130 14955.94 \n",
       "9996  No      No       711.5550 52992.38 \n",
       "9997  No      No       757.9629 19660.72 \n",
       "9998  No      No       845.4120 58636.16 \n",
       "9999  No      No      1569.0091 36669.11 \n",
       "10000 No      Yes      200.9222 16862.95 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(MASS)\n",
    "library(ISLR)\n",
    "library(class)\n",
    "library(boot)\n",
    "library(dplyr)\n",
    "Default\n",
    "#cor(Default)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Split the data in tain and test with one random sample of 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "sample_num <-sample(1:nrow(Default), size= 0.8 * nrow(Default), replace=F)\n",
    "\n",
    "train <- Default[sample_num, ]\n",
    "test <- Default[-sample_num, ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " When splitting the data in tain and test, the thing that should be considered is that the categories should be splitted in a way that they are not biased. For example, in this dataset(The Default dataset), the training set can contain only data of non-student (student=='No') while the test set only contains the data of students. In this kind of situation, train set and test set cannot be properly studied and tested respectively as the both are incomplete.\n",
    "To prevent this 'tilted to one side' situation, some methods can be taken such as randomly shuffling the entire data and then splitting the train-and-test set. That is the reason why we split the Default data with 'random sample' of 80/20.\n",
    "\n",
    " Also, when splitting the dataset into train and test set, the ratio of train/test set should be considered. (Here, the ratio is 80/20.) Because, for example, the model cannot be trained effectively enough with too small training set. On the other hand, with too small test set, the calculations(accuracy, recall,fscore,,,,etc) have high possibility to differ greatly from each other, which represents that the result cannot be fully trusted.\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The code below is the function which prints out the confusion matrix of prediction for certain model, and return the calculations of that matrix(Precision, recall, and f.score).\n",
    "(Just to prevent the meaningless repeats of creating matrix and calculating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrx_and_stats<- function(pred,actual){ # the input= prediction and actual data for test set\n",
    "    #creating confusion matrix\n",
    "    confusion_matrix<-table(pred,actual)\n",
    "    print(confusion_matrix)\n",
    "    #calculating precision, recall, and f-score from the matrix\n",
    "    precision <- confusion_matrix[2,2] / sum(confusion_matrix[2,])\n",
    "    recall <- confusion_matrix[2,2] / sum(confusion_matrix[,2])\n",
    "    F.score <- 2*precision*recall/(precision+recall)\n",
    "    stats<-c(precision,recall,F.score)\n",
    "    return(stats)\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     actual\n",
      "pred    No  Yes\n",
      "  No  1928   46\n",
      "  Yes    6   20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.769230769230769</li>\n",
       "\t<li>0.303030303030303</li>\n",
       "\t<li>0.434782608695652</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.769230769230769\n",
       "\\item 0.303030303030303\n",
       "\\item 0.434782608695652\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.769230769230769\n",
       "2. 0.303030303030303\n",
       "3. 0.434782608695652\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.7692308 0.3030303 0.4347826"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# building model with training data\n",
    "glm.fit <- glm(default~income+balance+student,\n",
    "               data=train, family='binomial')\n",
    "# Predicting with test data\n",
    "glm.probs<- predict(glm.fit, newdata=test, type='response')\n",
    "# As glm.probs is not the response 'yes' and 'no',\n",
    "#I just turned it into the response prediction by creating glm.pred data\n",
    "glm.pred<- rep('No', nrow(test))\n",
    "glm.pred[glm.probs>0.5]='Yes'\n",
    "\n",
    "#Applying the function that I made above\n",
    "glm_stats<-mtrx_and_stats(glm.pred, test$default)\n",
    "glm_stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The precision, recall and F1-score are 0.793103448275862, 0.365079365079365, 0.5, respectively.\n",
    "\n",
    "#######Relevance of Predictors: glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = default ~ income + balance + student, family = \"binomial\", \n",
       "    data = train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.1526  -0.1404  -0.0558  -0.0199   3.7422  \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -1.097e+01  5.487e-01 -19.995   <2e-16 ***\n",
       "income       4.449e-06  9.095e-06   0.489   0.6248    \n",
       "balance      5.772e-03  2.605e-04  22.162   <2e-16 ***\n",
       "studentYes  -6.344e-01  2.621e-01  -2.421   0.0155 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 2340.6  on 7999  degrees of freedom\n",
       "Residual deviance: 1252.3  on 7996  degrees of freedom\n",
       "AIC: 1260.3\n",
       "\n",
       "Number of Fisher Scoring iterations: 8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(glm.fit)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In logistic regression, the relevance of predictor can be estimated by its p-value. Usually, when the p-value is less than 0.05, the predictor is considered relevance for the response. As you see the p-values, the income predictors seems to be less relevant to default, as its p-value is 0.6248, which is much larger than 0.05."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     actual\n",
      "pred    No  Yes\n",
      "  No  1929   51\n",
      "  Yes    5   15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.75</li>\n",
       "\t<li>0.227272727272727</li>\n",
       "\t<li>0.348837209302326</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.75\n",
       "\\item 0.227272727272727\n",
       "\\item 0.348837209302326\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.75\n",
       "2. 0.227272727272727\n",
       "3. 0.348837209302326\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.7500000 0.2272727 0.3488372"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# building model with training data\n",
    "lda.fit <- lda(default~income+balance+student,\n",
    "               data=train)\n",
    "# Predicting with test data\n",
    "lda.pred <- predict(lda.fit, newdata=test)\n",
    "# lda.class is the predicted classification(yes/no)\n",
    "lda.class=lda.pred$class\n",
    "\n",
    "#Applying the function that I made above\n",
    "lda_stats<-mtrx_and_stats(lda.class, test$default)\n",
    "lda_stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The precision, recall and F1-score are 0.80952380952381, 0.26984126984127, 0.404761904761905, respectively.\n",
    "\n",
    "######## Relevance of predictors: lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(default ~ income + balance + student, data = train)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "      No      Yes \n",
       "0.966625 0.033375 \n",
       "\n",
       "Group means:\n",
       "      income   balance studentYes\n",
       "No  33561.60  804.6836  0.2890211\n",
       "Yes 32151.98 1750.1822  0.3820225\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "                     LD1\n",
       "income      4.437213e-06\n",
       "balance     2.246048e-03\n",
       "studentYes -1.513689e-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda.fit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Searching for the clear criteria, there was no such way to clearly estimate the exact importance of predictors(Based on current knowledge.)\n",
    "However, there exists some ways to 'guess' the approxiamte amount of importance of variables like following;\n",
    " 1. variables with large absolute values in the scaling(Coefficients of linear discriminants) are more likely to influence the response.\n",
    " 2. variables with large differences between group means tend to have more significant importance on the response.\n",
    "According to these two tendencies, first, all three variables do have Coefficients of linear discriminants which are close to zero. It gives us insight that it is likely to serve as the basis for the hypothesis that variables do not significantly affect the target(default).(Remember this is only a guess.) Next, comparing the variable group mean, balance seems to have meaningful difference, while income, just like the analysis from logistic regression approach, does not seem to have significant difference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Qaufratic fit (Omitted extra description as the process is same with lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     actual\n",
      "pred    No  Yes\n",
      "  No  1928   49\n",
      "  Yes    6   17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.739130434782609</li>\n",
       "\t<li>0.257575757575758</li>\n",
       "\t<li>0.382022471910112</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.739130434782609\n",
       "\\item 0.257575757575758\n",
       "\\item 0.382022471910112\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.739130434782609\n",
       "2. 0.257575757575758\n",
       "3. 0.382022471910112\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.7391304 0.2575758 0.3820225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# building model with training data\n",
    "qda.fit <- qda(default~income+balance+student, data=train)\n",
    "# Predicting with test data\n",
    "qda.class<- predict(qda.fit, newdata=test)$class\n",
    "#Applying the function\n",
    "qda_stats<-mtrx_and_stats(qda.class, test$default)\n",
    "qda_stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The precision, recall and F1-score are 0.807692307692308, 0.333333333333333, 0.471910112359551, respectively.\n",
    "\n",
    "\n",
    "#####Relevance of Predictors: qda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "qda(default ~ income + balance + student, data = train)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "      No      Yes \n",
       "0.966625 0.033375 \n",
       "\n",
       "Group means:\n",
       "      income   balance studentYes\n",
       "No  33561.60  804.6836  0.2890211\n",
       "Yes 32151.98 1750.1822  0.3820225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qda.fit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Similar with lda, qda follows the tendency below:\n",
    "\"Variables with large differences between group means tend to have more significant importance on the response.\"\n",
    "According to the result qda.fit, balance tends to have relatively strong relevance on response as its difference between group means 'yes/no' is quite large.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     actual\n",
      "pred    No  Yes\n",
      "  No  1915   49\n",
      "  Yes   22   14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.388888888888889</li>\n",
       "\t<li>0.222222222222222</li>\n",
       "\t<li>0.282828282828283</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.388888888888889\n",
       "\\item 0.222222222222222\n",
       "\\item 0.282828282828283\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.388888888888889\n",
       "2. 0.222222222222222\n",
       "3. 0.282828282828283\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.3888889 0.2222222 0.2828283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As student predictor is categorial(qualatative, not quantatative),\n",
    "# it should be turned into numbers.\n",
    "# So I just created copy dataframe of default just for knn.\n",
    "Default_for_knn<-data.frame(Default)\n",
    "\n",
    "Default_for_knn$student <- ifelse(Default_for_knn$student=='Yes', 1,0)\n",
    "\n",
    "# As it turned into numbers, the student column of train and test set should be also changed,\n",
    "# so I just repeated the process of splitting the changed dataset into train and test\n",
    "# like following\n",
    "sample_num_knn <-sample(1:nrow(Default_for_knn), size= 0.8 * nrow(Default_for_knn), replace=F)\n",
    "train_knn <- Default_for_knn[sample_num_knn, ]\n",
    "test_knn <- Default_for_knn[-sample_num_knn, ]\n",
    "\n",
    "# predicting with the model\n",
    "knn.pred<- knn(train_knn[,2:4], test_knn[,2:4],\n",
    "               cl=train_knn$default, k=3)\n",
    "\n",
    "#trying putting other numbers into k, 3 was the optimal which has highest stats result\n",
    "knn_stats<- mtrx_and_stats(knn.pred,test_knn$default)\n",
    "knn_stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The precision, recall and F1-score are 0.533333333333333, 0.266666666666667, 0.355555555555556, respectively."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############ Relevance of Predictors: knn\n",
    "KNN is a nonparametric method by looking at the nearest k observations and classifying them according to their characteristics. As the purpose of knn is to reflect the current state of the data, the importance of variables loses its meaning. In other words, for example since if all k of the points a new point is close to are in one class, it doesn't matter which of the variables it was close to, just that it was close to enough of them in the p-dimensional space to be a neighbor. Therefore, we cannot figure out the relevance of predictors from knn approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#calculate precision, recall and F1-score\n",
    "To take the best approach among these four models, comparing process should be preceeded.\n",
    "Therefore, to compare their precision, recall and f1 score, I just created a dataframe which contains calculations for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>glm_stats</th><th scope=col>lda_stats</th><th scope=col>qda_stats</th><th scope=col>knn_stats</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>precision</th><td>0.7692308</td><td>0.7500000</td><td>0.7391304</td><td>0.3888889</td></tr>\n",
       "\t<tr><th scope=row>recall</th><td>0.3030303</td><td>0.2272727</td><td>0.2575758</td><td>0.2222222</td></tr>\n",
       "\t<tr><th scope=row>f-score</th><td>0.4347826</td><td>0.3488372</td><td>0.3820225</td><td>0.2828283</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & glm\\_stats & lda\\_stats & qda\\_stats & knn\\_stats\\\\\n",
       "\\hline\n",
       "\tprecision & 0.7692308 & 0.7500000 & 0.7391304 & 0.3888889\\\\\n",
       "\trecall & 0.3030303 & 0.2272727 & 0.2575758 & 0.2222222\\\\\n",
       "\tf-score & 0.4347826 & 0.3488372 & 0.3820225 & 0.2828283\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | glm_stats | lda_stats | qda_stats | knn_stats |\n",
       "|---|---|---|---|---|\n",
       "| precision | 0.7692308 | 0.7500000 | 0.7391304 | 0.3888889 |\n",
       "| recall | 0.3030303 | 0.2272727 | 0.2575758 | 0.2222222 |\n",
       "| f-score | 0.4347826 | 0.3488372 | 0.3820225 | 0.2828283 |\n",
       "\n"
      ],
      "text/plain": [
       "          glm_stats lda_stats qda_stats knn_stats\n",
       "precision 0.7692308 0.7500000 0.7391304 0.3888889\n",
       "recall    0.3030303 0.2272727 0.2575758 0.2222222\n",
       "f-score   0.4347826 0.3488372 0.3820225 0.2828283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with the highest precision: glm_stats \n",
      " model with the highest recall: glm_stats \n",
      " model with the highest f1-score: glm_stats"
     ]
    }
   ],
   "source": [
    "df=data.frame(glm_stats,lda_stats,qda_stats,knn_stats)\n",
    "dimnames(df)=list(row=c(\"precision\",\"recall\",'f-score'),col=names(df));df\n",
    "# selecting the model which has the highest score\n",
    "#on each calculation(precision, recall and f1-score)\n",
    "\n",
    "precision_first <- names(df)[max.col(replace(df, is.na(df),0),ties.method = \"first\")][1]\n",
    "recall_first <- names(df)[max.col(replace(df, is.na(df),0),ties.method = \"first\")][2]\n",
    "f1_score_first <- names(df)[max.col(replace(df, is.na(df),0),ties.method = \"first\")][3]\n",
    "cat('model with the highest precision:',precision_first,\n",
    "   '\\n model with the highest recall:',recall_first,\n",
    "   '\\n model with the highest f1-score:',f1_score_first)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As logistic regression has the highest calculation results on precision, recall and f1-score, the conclusion is the logistic regression model is the best approach among these four models, in aspect of its calculation results.\n",
    "\n",
    "#############################################################################################\n",
    "<Discuss differences between the approaches>\n",
    "Based on the preceding content, the characteristics and differences of the four models can be summarized as follows:\n",
    "1.Logistic Regression: A statistical model that in its basic form uses a logistic function to model a binary dependent variable. Could estimate approximate importance of variables by p-values, and ranked highest(not always but in this case) on recall, precision, and f1-score.\n",
    "2.LDA: The model which goal is to 'project' the data on a specific axis and then find a straight line that can distinguish between the two categories. Could not clearly estimate the importance of variables(based on current knowledge), but could guess it by the difference between group means and coefficients of linear discriminants. \n",
    "3.QDA: Different with LDA, QDA gives each k class its own covariance matrix. Similar with LDA, the importance of variables can be guessed by the difference between group means.\n",
    "4.KNN: Has characteristic that it can show good performance when the actual decision foundation is non-linear, according to textbook. But cannot judge the importance of variables on response. Also, the default knn model worked worst among four models on classifying default/non-default.\n",
    "#############################################################################################\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Take the best approach and compare it with its own using a 10-fold crossvalidation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>default</th><th scope=col>student</th><th scope=col>balance</th><th scope=col>income</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 729.5265</td><td>44361.625</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 817.1804</td><td>12106.135</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1073.5492</td><td>31767.139</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 529.2506</td><td>35704.494</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 785.6559</td><td>38463.496</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 919.5885</td><td> 7491.559</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 825.5133</td><td>24905.227</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 808.6675</td><td>17600.451</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1161.0579</td><td>37468.529</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>   0.0000</td><td>29275.268</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td>   0.0000</td><td>21871.073</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td>1220.5838</td><td>13268.562</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 237.0451</td><td>28251.695</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 606.7423</td><td>44994.556</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1112.9684</td><td>23810.174</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 286.2326</td><td>45042.413</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>   0.0000</td><td>50265.312</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 527.5402</td><td>17636.540</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 485.9369</td><td>61566.106</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1095.0727</td><td>26464.631</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 228.9525</td><td>50500.182</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 954.2618</td><td>32457.509</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1055.9566</td><td>51317.883</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 641.9844</td><td>30466.103</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 773.2117</td><td>34353.314</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 855.0085</td><td>25211.332</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 642.9997</td><td>41473.512</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1454.8633</td><td>32189.095</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 615.7043</td><td>39376.395</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td>1119.5694</td><td>16556.070</td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td>1294.5004</td><td>25687.33 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 180.6201</td><td>20975.56 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 755.4328</td><td>14455.87 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 876.1190</td><td>37668.37 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 933.3320</td><td>26051.40 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 908.3159</td><td>21287.94 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 218.4176</td><td>25401.13 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 915.4398</td><td>16624.34 </td></tr>\n",
       "\t<tr><td>Yes      </td><td>No       </td><td>2202.4624</td><td>47287.26 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 173.2492</td><td>30697.25 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 770.0157</td><td>13684.79 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 739.4180</td><td>40656.95 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 623.5261</td><td>59441.31 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 506.6255</td><td>49861.00 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 875.2416</td><td>52861.74 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 842.9494</td><td>39957.13 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 401.3327</td><td>15332.02 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1092.9066</td><td>45479.47 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>   0.0000</td><td>41740.69 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 999.2811</td><td>20013.35 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 372.3792</td><td>25374.90 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 658.7996</td><td>54802.08 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1111.6473</td><td>45490.68 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 938.8362</td><td>56633.45 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 172.4130</td><td>14955.94 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 711.5550</td><td>52992.38 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 757.9629</td><td>19660.72 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 845.4120</td><td>58636.16 </td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1569.0091</td><td>36669.11 </td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 200.9222</td><td>16862.95 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " default & student & balance & income\\\\\n",
       "\\hline\n",
       "\t No        & No        &  729.5265 & 44361.625\\\\\n",
       "\t No        & Yes       &  817.1804 & 12106.135\\\\\n",
       "\t No        & No        & 1073.5492 & 31767.139\\\\\n",
       "\t No        & No        &  529.2506 & 35704.494\\\\\n",
       "\t No        & No        &  785.6559 & 38463.496\\\\\n",
       "\t No        & Yes       &  919.5885 &  7491.559\\\\\n",
       "\t No        & No        &  825.5133 & 24905.227\\\\\n",
       "\t No        & Yes       &  808.6675 & 17600.451\\\\\n",
       "\t No        & No        & 1161.0579 & 37468.529\\\\\n",
       "\t No        & No        &    0.0000 & 29275.268\\\\\n",
       "\t No        & Yes       &    0.0000 & 21871.073\\\\\n",
       "\t No        & Yes       & 1220.5838 & 13268.562\\\\\n",
       "\t No        & No        &  237.0451 & 28251.695\\\\\n",
       "\t No        & No        &  606.7423 & 44994.556\\\\\n",
       "\t No        & No        & 1112.9684 & 23810.174\\\\\n",
       "\t No        & No        &  286.2326 & 45042.413\\\\\n",
       "\t No        & No        &    0.0000 & 50265.312\\\\\n",
       "\t No        & Yes       &  527.5402 & 17636.540\\\\\n",
       "\t No        & No        &  485.9369 & 61566.106\\\\\n",
       "\t No        & No        & 1095.0727 & 26464.631\\\\\n",
       "\t No        & No        &  228.9525 & 50500.182\\\\\n",
       "\t No        & No        &  954.2618 & 32457.509\\\\\n",
       "\t No        & No        & 1055.9566 & 51317.883\\\\\n",
       "\t No        & No        &  641.9844 & 30466.103\\\\\n",
       "\t No        & No        &  773.2117 & 34353.314\\\\\n",
       "\t No        & No        &  855.0085 & 25211.332\\\\\n",
       "\t No        & No        &  642.9997 & 41473.512\\\\\n",
       "\t No        & No        & 1454.8633 & 32189.095\\\\\n",
       "\t No        & No        &  615.7043 & 39376.395\\\\\n",
       "\t No        & Yes       & 1119.5694 & 16556.070\\\\\n",
       "\t ... & ... & ... & ...\\\\\n",
       "\t No        & Yes       & 1294.5004 & 25687.33 \\\\\n",
       "\t No        & Yes       &  180.6201 & 20975.56 \\\\\n",
       "\t No        & No        &  755.4328 & 14455.87 \\\\\n",
       "\t No        & No        &  876.1190 & 37668.37 \\\\\n",
       "\t No        & Yes       &  933.3320 & 26051.40 \\\\\n",
       "\t No        & No        &  908.3159 & 21287.94 \\\\\n",
       "\t No        & No        &  218.4176 & 25401.13 \\\\\n",
       "\t No        & Yes       &  915.4398 & 16624.34 \\\\\n",
       "\t Yes       & No        & 2202.4624 & 47287.26 \\\\\n",
       "\t No        & No        &  173.2492 & 30697.25 \\\\\n",
       "\t No        & Yes       &  770.0157 & 13684.79 \\\\\n",
       "\t No        & No        &  739.4180 & 40656.95 \\\\\n",
       "\t No        & No        &  623.5261 & 59441.31 \\\\\n",
       "\t No        & No        &  506.6255 & 49861.00 \\\\\n",
       "\t No        & No        &  875.2416 & 52861.74 \\\\\n",
       "\t No        & No        &  842.9494 & 39957.13 \\\\\n",
       "\t No        & Yes       &  401.3327 & 15332.02 \\\\\n",
       "\t No        & No        & 1092.9066 & 45479.47 \\\\\n",
       "\t No        & No        &    0.0000 & 41740.69 \\\\\n",
       "\t No        & Yes       &  999.2811 & 20013.35 \\\\\n",
       "\t No        & No        &  372.3792 & 25374.90 \\\\\n",
       "\t No        & No        &  658.7996 & 54802.08 \\\\\n",
       "\t No        & No        & 1111.6473 & 45490.68 \\\\\n",
       "\t No        & No        &  938.8362 & 56633.45 \\\\\n",
       "\t No        & Yes       &  172.4130 & 14955.94 \\\\\n",
       "\t No        & No        &  711.5550 & 52992.38 \\\\\n",
       "\t No        & No        &  757.9629 & 19660.72 \\\\\n",
       "\t No        & No        &  845.4120 & 58636.16 \\\\\n",
       "\t No        & No        & 1569.0091 & 36669.11 \\\\\n",
       "\t No        & Yes       &  200.9222 & 16862.95 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| default | student | balance | income |\n",
       "|---|---|---|---|\n",
       "| No        | No        |  729.5265 | 44361.625 |\n",
       "| No        | Yes       |  817.1804 | 12106.135 |\n",
       "| No        | No        | 1073.5492 | 31767.139 |\n",
       "| No        | No        |  529.2506 | 35704.494 |\n",
       "| No        | No        |  785.6559 | 38463.496 |\n",
       "| No        | Yes       |  919.5885 |  7491.559 |\n",
       "| No        | No        |  825.5133 | 24905.227 |\n",
       "| No        | Yes       |  808.6675 | 17600.451 |\n",
       "| No        | No        | 1161.0579 | 37468.529 |\n",
       "| No        | No        |    0.0000 | 29275.268 |\n",
       "| No        | Yes       |    0.0000 | 21871.073 |\n",
       "| No        | Yes       | 1220.5838 | 13268.562 |\n",
       "| No        | No        |  237.0451 | 28251.695 |\n",
       "| No        | No        |  606.7423 | 44994.556 |\n",
       "| No        | No        | 1112.9684 | 23810.174 |\n",
       "| No        | No        |  286.2326 | 45042.413 |\n",
       "| No        | No        |    0.0000 | 50265.312 |\n",
       "| No        | Yes       |  527.5402 | 17636.540 |\n",
       "| No        | No        |  485.9369 | 61566.106 |\n",
       "| No        | No        | 1095.0727 | 26464.631 |\n",
       "| No        | No        |  228.9525 | 50500.182 |\n",
       "| No        | No        |  954.2618 | 32457.509 |\n",
       "| No        | No        | 1055.9566 | 51317.883 |\n",
       "| No        | No        |  641.9844 | 30466.103 |\n",
       "| No        | No        |  773.2117 | 34353.314 |\n",
       "| No        | No        |  855.0085 | 25211.332 |\n",
       "| No        | No        |  642.9997 | 41473.512 |\n",
       "| No        | No        | 1454.8633 | 32189.095 |\n",
       "| No        | No        |  615.7043 | 39376.395 |\n",
       "| No        | Yes       | 1119.5694 | 16556.070 |\n",
       "| ... | ... | ... | ... |\n",
       "| No        | Yes       | 1294.5004 | 25687.33  |\n",
       "| No        | Yes       |  180.6201 | 20975.56  |\n",
       "| No        | No        |  755.4328 | 14455.87  |\n",
       "| No        | No        |  876.1190 | 37668.37  |\n",
       "| No        | Yes       |  933.3320 | 26051.40  |\n",
       "| No        | No        |  908.3159 | 21287.94  |\n",
       "| No        | No        |  218.4176 | 25401.13  |\n",
       "| No        | Yes       |  915.4398 | 16624.34  |\n",
       "| Yes       | No        | 2202.4624 | 47287.26  |\n",
       "| No        | No        |  173.2492 | 30697.25  |\n",
       "| No        | Yes       |  770.0157 | 13684.79  |\n",
       "| No        | No        |  739.4180 | 40656.95  |\n",
       "| No        | No        |  623.5261 | 59441.31  |\n",
       "| No        | No        |  506.6255 | 49861.00  |\n",
       "| No        | No        |  875.2416 | 52861.74  |\n",
       "| No        | No        |  842.9494 | 39957.13  |\n",
       "| No        | Yes       |  401.3327 | 15332.02  |\n",
       "| No        | No        | 1092.9066 | 45479.47  |\n",
       "| No        | No        |    0.0000 | 41740.69  |\n",
       "| No        | Yes       |  999.2811 | 20013.35  |\n",
       "| No        | No        |  372.3792 | 25374.90  |\n",
       "| No        | No        |  658.7996 | 54802.08  |\n",
       "| No        | No        | 1111.6473 | 45490.68  |\n",
       "| No        | No        |  938.8362 | 56633.45  |\n",
       "| No        | Yes       |  172.4130 | 14955.94  |\n",
       "| No        | No        |  711.5550 | 52992.38  |\n",
       "| No        | No        |  757.9629 | 19660.72  |\n",
       "| No        | No        |  845.4120 | 58636.16  |\n",
       "| No        | No        | 1569.0091 | 36669.11  |\n",
       "| No        | Yes       |  200.9222 | 16862.95  |\n",
       "\n"
      ],
      "text/plain": [
       "      default student balance   income   \n",
       "1     No      No       729.5265 44361.625\n",
       "2     No      Yes      817.1804 12106.135\n",
       "3     No      No      1073.5492 31767.139\n",
       "4     No      No       529.2506 35704.494\n",
       "5     No      No       785.6559 38463.496\n",
       "6     No      Yes      919.5885  7491.559\n",
       "7     No      No       825.5133 24905.227\n",
       "8     No      Yes      808.6675 17600.451\n",
       "9     No      No      1161.0579 37468.529\n",
       "10    No      No         0.0000 29275.268\n",
       "11    No      Yes        0.0000 21871.073\n",
       "12    No      Yes     1220.5838 13268.562\n",
       "13    No      No       237.0451 28251.695\n",
       "14    No      No       606.7423 44994.556\n",
       "15    No      No      1112.9684 23810.174\n",
       "16    No      No       286.2326 45042.413\n",
       "17    No      No         0.0000 50265.312\n",
       "18    No      Yes      527.5402 17636.540\n",
       "19    No      No       485.9369 61566.106\n",
       "20    No      No      1095.0727 26464.631\n",
       "21    No      No       228.9525 50500.182\n",
       "22    No      No       954.2618 32457.509\n",
       "23    No      No      1055.9566 51317.883\n",
       "24    No      No       641.9844 30466.103\n",
       "25    No      No       773.2117 34353.314\n",
       "26    No      No       855.0085 25211.332\n",
       "27    No      No       642.9997 41473.512\n",
       "28    No      No      1454.8633 32189.095\n",
       "29    No      No       615.7043 39376.395\n",
       "30    No      Yes     1119.5694 16556.070\n",
       "...   ...     ...     ...       ...      \n",
       "9971  No      Yes     1294.5004 25687.33 \n",
       "9972  No      Yes      180.6201 20975.56 \n",
       "9973  No      No       755.4328 14455.87 \n",
       "9974  No      No       876.1190 37668.37 \n",
       "9975  No      Yes      933.3320 26051.40 \n",
       "9976  No      No       908.3159 21287.94 \n",
       "9977  No      No       218.4176 25401.13 \n",
       "9978  No      Yes      915.4398 16624.34 \n",
       "9979  Yes     No      2202.4624 47287.26 \n",
       "9980  No      No       173.2492 30697.25 \n",
       "9981  No      Yes      770.0157 13684.79 \n",
       "9982  No      No       739.4180 40656.95 \n",
       "9983  No      No       623.5261 59441.31 \n",
       "9984  No      No       506.6255 49861.00 \n",
       "9985  No      No       875.2416 52861.74 \n",
       "9986  No      No       842.9494 39957.13 \n",
       "9987  No      Yes      401.3327 15332.02 \n",
       "9988  No      No      1092.9066 45479.47 \n",
       "9989  No      No         0.0000 41740.69 \n",
       "9990  No      Yes      999.2811 20013.35 \n",
       "9991  No      No       372.3792 25374.90 \n",
       "9992  No      No       658.7996 54802.08 \n",
       "9993  No      No      1111.6473 45490.68 \n",
       "9994  No      No       938.8362 56633.45 \n",
       "9995  No      Yes      172.4130 14955.94 \n",
       "9996  No      No       711.5550 52992.38 \n",
       "9997  No      No       757.9629 19660.72 \n",
       "9998  No      No       845.4120 58636.16 \n",
       "9999  No      No      1569.0091 36669.11 \n",
       "10000 No      Yes      200.9222 16862.95 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.0213639200961899"
      ],
      "text/latex": [
       "0.0213639200961899"
      ],
      "text/markdown": [
       "0.0213639200961899"
      ],
      "text/plain": [
       "[1] 0.02136392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10-fold cross validation\n",
    "glm.fit_cv <- glm(default~income+balance+student, data=Default, family='binomial')\n",
    "cv.error.10 = cv.glm(Default,glm.fit_cv,K=10)$delta[1] #set the k as 10, so the number of fold=10\n",
    "cv.error.10 #the Cv error(average of MSEs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To compare the Cv error with MSE of logistic regression(the old best model), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Cv error of 10-fold cross validation has higher error than MSE of the old best model'"
      ],
      "text/latex": [
       "'Cv error of 10-fold cross validation has higher error than MSE of the old best model'"
      ],
      "text/markdown": [
       "'Cv error of 10-fold cross validation has higher error than MSE of the old best model'"
      ],
      "text/plain": [
       "[1] \"Cv error of 10-fold cross validation has higher error than MSE of the old best model\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(123)\n",
    "#sample_num <-sample(1:nrow(Default), size= 0.8 * nrow(Default), replace=F)\n",
    "#train <- Default[sample_num, ]\n",
    "#test <- Default[-sample_num, ]\n",
    "\n",
    "glm.fit <- glm(default~income+balance+student,\n",
    "               data=train, family='binomial') #the old-best model\n",
    "glm.probs <- predict(glm.fit, newdata=test, type='response')\n",
    "test$default<- ifelse(test$default=='Yes',1,0)\n",
    "MSE_old_best<-mean((test$default-glm.probs)^2)\n",
    "test$default<- ifelse(test$default==1,'Yes','No')\n",
    "\n",
    "ifelse(cv.error.10 > MSE_old_best,\n",
    "       'Cv error of 10-fold cross validation has higher error than MSE of the old best model',\n",
    "       'Cv error of 10-fold cross validation has lower error than MSE of the old best model')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fixing the seed to '123', the Cv error from k-fold cross validation is larger than the MSE of old-best model.\n",
    "However, without fixing the seed into certain number, the MSE of old best model changes with quite large deviation. (You can check it by adding '#' on the first line and setting the sample again with the same ratio of old model, by deleting all the '#'s from the second line to the fourth.)\n",
    "It sometimes get bigger than the Cv, and sometimes not. The reason why this situation happens is that the old best model, (the model without k-fold cross validation approach) has possibility to be 'overfitted' sometimes, since it trains and tests the dataset only once. Therefore, it can be concluded that k-fold cross validation approach is 'more stable' than the old model, in aspect of its role of assessing the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "51"
      ],
      "text/latex": [
       "51"
      ],
      "text/markdown": [
       "51"
      ],
      "text/plain": [
       "[1] 51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "49"
      ],
      "text/latex": [
       "49"
      ],
      "text/markdown": [
       "49"
      ],
      "text/plain": [
       "[1] 49"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Extra Researching\n",
    "\n",
    "tenfold<-0\n",
    "old_best<-0\n",
    "\n",
    "for (i in 1:100){\n",
    "    sample_num <-sample(1:nrow(Default), size= 0.8 * nrow(Default), replace=F)\n",
    "    train <- Default[sample_num, ]\n",
    "    test <- Default[-sample_num, ]\n",
    "    glm.fit <- glm(default~income+balance+student,\n",
    "                   data=train, family='binomial') #the old-best model\n",
    "    glm.probs <- predict(glm.fit, newdata=test, type='response')\n",
    "    test$default<- ifelse(test$default=='Yes',1,0)\n",
    "    MSE_old_best<-mean((test$default-glm.probs)^2)\n",
    "    test$default<- ifelse(test$default==1,'Yes','No')\n",
    "    if (cv.error.10 > MSE_old_best){\n",
    "        tenfold=tenfold+1\n",
    "        }else{\n",
    "        old_best=old_best+1\n",
    "    }}\n",
    "tenfold # the number of times that Cv error is higher than MSE of old best model\n",
    "old_best # the number of times that Cv error is lower than MSE of old best model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Plus, to compare the number of times when either of the approach has higher error, I just repeated the comparing process 100 times. As you see, the number of times does not seem to have meaningful difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
